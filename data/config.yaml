# vLLM Benchmark Configuration

# Model to benchmark


# Single test configuration
test:

  baseline:
    name: "exp1"
    description: "Basic vLLM performance test"
    model: "Qwen/Qwen2.5-0.5B"
    runs: 1

    # vLLM server parameters
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16 
      max_model_len: 2048
      max_num_batched_tokens: 2048
      max_num_seqs: 256
      long_prefill_token_threshold: 1000000
      seed: 42  # Set a seed for reproducibility

    # Benchmark parameters
    benchmark:
      backend: "vllm"
      dataset_name: "sharegpt"
      dataset_path: "ShareGPT_V3_unfiltered_cleaned_split.json"
      num_prompts: 100
      request_rate: 16
      sharegpt_output_len: 0 # Set to 0 for no output length limit
      temperature: 0.0
      seed: 42  # Set a seed for reproducibility

  baseline2:
    name: "exp2"
    description: "Basic vLLM performance test"
    model: "Qwen/Qwen2.5-0.5B"
    runs: 1

    # vLLM server parameters
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 2048
      max_num_seqs: 256
      long_prefill_token_threshold: 1000000
      seed: 42

    # Benchmark parameters
    benchmark:
      backend: "vllm"
      dataset_name: "sharegpt"
      dataset_path: "ShareGPT_V3_unfiltered_cleaned_split.json"
      num_prompts: 100
      request_rate: 30
      sharegpt_output_len: 0 # Set to 0 for no output length limit
      temperature: 0.0
      seed: 42

  baseline3:
      name: "exp3"
      description: "Basic vLLM performance test"
      model: "Qwen/Qwen2.5-0.5B"
      runs: 1

      # vLLM server parameters
      vllm:
        gpu_memory_utilization: 0.9
        enable_prefix_caching: true
        disable_log_requests: false
        block_size: 16
        max_model_len: 2048
        max_num_batched_tokens: 2048
        max_num_seqs: 256
        long_prefill_token_threshold: 1000000
        seed: 42

      # Benchmark parameters
      benchmark:
        backend: "vllm"
        dataset_name: "sharegpt"
        dataset_path: "ShareGPT_V3_unfiltered_cleaned_split.json"
        num_prompts: 100
        request_rate: 64
        sharegpt_output_len: 0 # Set to 0 for no output length limit
        temperature: 0.0
        seed: 42
