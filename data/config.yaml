# vLLM Benchmark Configuration

# Model to benchmark


# Single test configuration
test:
  # baseline0:
  #   name: "test"
  #   description: "Basic vLLM performance test"
  #   model: "Qwen/Qwen2.5-0.5B"
  #   runs: 1

  #   # vLLM server parameters
  #   vllm:
  #     gpu_memory_utilization: 0.9
  #     enable_prefix_caching: true
  #     disable_log_requests: false
  #     block_size: 16
  #     max_model_len: 2048
  #     max_num_batched_tokens: 2048
  #     max_num_seqs: 256
  #     long_prefill_token_threshold: 1000000
  #     seed: 42  # Set a seed for reproducibility

  #   # Benchmark parameters
  #   benchmark:
  #     backend: "vllm"
  #     dataset_name: "sharegpt"
  #     dataset_path: "ShareGPT_V3_unfiltered_cleaned_split.json"
  #     num_prompts: 1
  #     request_rate: 1
  #     temperature: 0.0
  #     seed: 42  # Set a seed for reproducibility

  baseline:
    name: "exp1"
    description: "Basic vLLM performance test"
    model: "Qwen/Qwen2.5-0.5B"
    runs: 1

    # vLLM server parameters
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 2048
      max_num_seqs: 256
      long_prefill_token_threshold: 1000000
      seed: 42  # Set a seed for reproducibility

    # Benchmark parameters
    benchmark:
      backend: "vllm"
      dataset_name: "sharegpt"
      dataset_path: "ShareGPT_V3_unfiltered_cleaned_split.json"
      num_prompts: 5000
      request_rate: 25
      temperature: 0.0
      seed: 42  # Set a seed for reproducibility

  baseline2:
    name: "exp2"
    description: "Basic vLLM performance test"
    model: "Qwen/Qwen2.5-0.5B"
    runs: 1

    # vLLM server parameters
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 2048
      max_num_seqs: 256
      long_prefill_token_threshold: 1000000
      seed: 42

    # Benchmark parameters
    benchmark:
      backend: "vllm"
      dataset_name: "sharegpt"
      dataset_path: "ShareGPT_V3_unfiltered_cleaned_split.json"
      num_prompts: 5000
      request_rate: 38
      temperature: 0.0
      seed: 42

  baseline3:
      name: "exp3"
      description: "Basic vLLM performance test"
      model: "Qwen/Qwen2.5-0.5B"
      runs: 1

      # vLLM server parameters
      vllm:
        gpu_memory_utilization: 0.9
        enable_prefix_caching: true
        disable_log_requests: false
        block_size: 16
        max_model_len: 2048
        max_num_batched_tokens: 2048
        max_num_seqs: 256
        long_prefill_token_threshold: 1000000
        seed: 42

      # Benchmark parameters
      benchmark:
        backend: "vllm"
        dataset_name: "sharegpt"
        dataset_path: "ShareGPT_V3_unfiltered_cleaned_split.json"
        num_prompts: 5000
        request_rate: 30
        temperature: 0.0
        seed: 42

  baseline5:
    name: "exp4"
    description: "Basic vLLM performance test"
    model: "Qwen/Qwen2.5-0.5B"
    runs: 1

    # vLLM server parameters
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 2048
      max_num_seqs: 256
      long_prefill_token_threshold: 1000000
      seed: 42

    # Benchmark parameters
    benchmark:
      backend: "vllm"
      dataset_name: "sharegpt"
      dataset_path: "ShareGPT_V3_unfiltered_cleaned_split.json"
      num_prompts: 5000
      request_rate: 32
      temperature: 0.0
      seed: 42
    
  baseline6:
    name: "exp5"
    description: "Basic vLLM performance test"
    model: "Qwen/Qwen2.5-0.5B"
    runs: 1

    # vLLM server parameters
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 2048
      max_num_seqs: 256
      long_prefill_token_threshold: 1000000
      seed: 42

    # Benchmark parameters
    benchmark:
      backend: "vllm"
      dataset_name: "sharegpt"
      dataset_path: "ShareGPT_V3_unfiltered_cleaned_split.json"
      num_prompts: 5000
      request_rate: 35
      temperature: 0.0
      seed: 42

  baseline7:
    name: "exp5"
    description: "Basic vLLM performance test"
    model: "Qwen/Qwen2.5-0.5B"
    runs: 1

    # vLLM server parameters
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 2048
      max_num_seqs: 256
      long_prefill_token_threshold: 1000000
      seed: 42

    # Benchmark parameters
    benchmark:
      backend: "vllm"
      dataset_name: "sharegpt"
      dataset_path: "ShareGPT_V3_unfiltered_cleaned_split.json"
      num_prompts: 5000
      request_rate: 40
      temperature: 0.0
      seed: 42